{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from confluent_kafka import Producer\n",
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "import socket\n",
    "import configparser\n",
    "import json\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('reddit_api/reddit-cred.config')\n",
    "\n",
    "\n",
    "class Prepare:\n",
    "    def get_data_from_api(self,topic='dataengineering'):\n",
    "        base_url = 'https://www.reddit.com/'\n",
    "        auth = requests.auth.HTTPBasicAuth(\n",
    "            config.get('REDDIT','user_id'),\n",
    "            config.get('REDDIT','secret')\n",
    "        )\n",
    "        \n",
    "        data = {\"grant_type\": \"password\", \n",
    "                    \"username\": config.get('REDDIT','username'),\n",
    "                    \"password\": config.get('REDDIT','password')}\n",
    "\n",
    "        r = requests.post(base_url + 'api/v1/access_token',\n",
    "                            data=data,\n",
    "                        headers={'user-agent': 'reddit-data'},\n",
    "                auth=auth)\n",
    "\n",
    "        token = 'bearer ' + r.json()['access_token']\n",
    "\n",
    "        base_url = 'https://oauth.reddit.com'\n",
    "        headers = {'Authorization': token, 'User-Agent': 'reddit-data'}\n",
    "\n",
    "        payload = {'q': f'r/{topic}', 'limit': 100, 'sort': 'new'}\n",
    "        response = requests.get(base_url + '/search', headers=headers, params=payload)\n",
    "        values = response.json()\n",
    "        return values\n",
    "\n",
    "    def format_data(self,values):\n",
    "        fetched_data = []\n",
    "        for i in range(len(values['data']['children'])):\n",
    "            data = dict()\n",
    "            data['TS'] = values['data']['children'][i]['data']['created']\n",
    "            data['TS_UTC'] = values['data']['children'][i]['data']['created_utc']\n",
    "            data['TITLE'] = values['data']['children'][i]['data']['title'].replace('\\n',' ').replace('/r','')\n",
    "            data['TEXT'] = values['data']['children'][i]['data']['selftext'].replace('\\n',' ').replace('/r','')\n",
    "            data['NSFW'] = values['data']['children'][i]['data']['over_18']\n",
    "            data['VOTE_RATIO'] = float(values['data']['children'][i]['data']['upvote_ratio'])\n",
    "            data['SCORE'] = float(values['data']['children'][i]['data']['score'])\n",
    "            data['URL'] = values['data']['children'][i]['data']['url']\n",
    "            data['USER_NAME'] = values['data']['children'][i]['data']['author']\n",
    "            data[\"WLS\"] = values['data']['children'][i]['data']['wls']\n",
    "\n",
    "            data[\"SUBREDDIT\"] = values['data']['children'][i]['data']['subreddit']\n",
    "            data[\"SUBREDDIT_TYPE\"] = values['data']['children'][i]['data']['subreddit_type']\n",
    "            data[\"SUBREDDIT_SUBSCRIBER_COUNT\"] = values['data']['children'][i]['data']['subreddit_subscribers']\n",
    "\n",
    "\n",
    "            \n",
    "            fetched_data.append(data)\n",
    "        return fetched_data\n",
    "\n",
    "    def produce_kafka(self,fetched_data,topic_name):\n",
    "\n",
    "        conf = {'bootstrap.servers': '192.168.89.83:9092',\n",
    "                'client.id': socket.gethostname()}\n",
    "\n",
    "        producer = Producer(conf)\n",
    "\n",
    "        # check topicname\n",
    "        if topic_name not in producer.list_topics().topics:\n",
    "            ac = AdminClient(conf)\n",
    "            topic_list = []\n",
    "            topic_list.append(NewTopic(topic=topic_name, num_partitions=1, replication_factor=1))\n",
    "            ac.create_topics(new_topics=topic_list, validate_only=False)\n",
    "\n",
    "        # create ack\n",
    "        def acked(err, msg):\n",
    "            if err is not None:\n",
    "                print(\"Failed to deliver message: %s: %s\" % (str(msg), str(err)))\n",
    "            else:\n",
    "                print(\"Message produced: %s\" % (str(msg)))\n",
    "\n",
    "        for item in range(len(fetched_data)):\n",
    "            producer.produce(topic_name, key=\"data\", value=\"{}\".format(json.dumps(fetched_data[item])), callback=acked)\n",
    "\n",
    "        producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPARK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType,StringType, FloatType\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "\n",
    "full_path_to_warehouse = 's3a://warehouse'\n",
    "# Bessie iÃ§in kullanacaÄŸÄ±mÄ±z branch \n",
    "branch_name = \"main\"\n",
    "# Nessie authentication tÃ¼rÃ¼. DiÄŸer seÃ§enekler (NONE, BEARER, OAUTH2 or AWS)\n",
    "auth_type = \"NONE\"\n",
    "# AWS S3 yerine MinIO kullandÄ±ÄŸÄ±mÄ±z iÃ§in. Spark'a amazona gitme burada kal demek iÃ§in.\n",
    "s3_endpoint = \"http://192.168.89.83:9000\"\n",
    "# MinIO'ya eriÅŸim iÃ§in. Bunlar root olarak docker-compose iÃ§inde belirtiliyor. Bu haliyle canlÄ± ortamlarda kullanÄ±lmamalÄ±dÄ±r.\n",
    "accessKeyId='dr6MxGKaZrSAh77gP8T0'\n",
    "secretAccessKey='czrPTk1rsUMzebG1UlJ0kki5VeGZPUYR5iIFX0af'\n",
    "nessie_url = \"http://192.168.89.83:19120/api/v1\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local\")\n",
    "    .appName(\"Spark UnÄ±ty Iceberg Demo\")\n",
    "    .config(\"spark.driver.memory\", \"16g\")\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "                 \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\\\n",
    "    .config('spark.jars.packages','org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.5.2,org.apache.hadoop:hadoop-aws:3.4.0')\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", accessKeyId)\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", secretAccessKey)\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", True)\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    # Spark Amazon S3 varsayÄ±lan API'sine deÄŸil lokaldeki MinIO'ya gitsin.\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", s3_endpoint)\n",
    "    .config(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\n",
    "\n",
    "    #Configuring Catalog\n",
    "    .config(\"spark.sql.catalog.nessie.uri\", nessie_url)\n",
    "    .config(\"spark.sql.catalog.nessie.ref\", branch_name)\n",
    "    .config(\"spark.sql.catalog.nessie.authentication.type\", auth_type)\n",
    "     .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.warehouse\", full_path_to_warehouse)\n",
    "    .config(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/27 00:41:41 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+---------------+-------------------+--------------------+--------------------+--------------+--------------------------+--------------------+-------------------+----------+-----+---------------+---+\n",
      "|            filename|SCORE|      SUBREDDIT|           ETL_DATE|               TITLE|                TEXT|SUBREDDIT_TYPE|SUBREDDIT_SUBSCRIBER_COUNT|                 URL|                 TS|VOTE_RATIO|TOPIC|      USER_NAME|WLS|\n",
      "+--------------------+-----+---------------+-------------------+--------------------+--------------------+--------------+--------------------------+--------------------+-------------------+----------+-----+---------------+---+\n",
      "|74090606-60cd-40a...|  4.0|       VietTalk|2024-10-19T16:00:14|Khi nhá»¯ng mÃ¡y cÃ y...|Há» lÃ  nhá»¯ng khÃ¡ch...|        public|                      4731|https://www.reddi...|2024-10-19T15:41:49|       1.0|  thy|fillapdesehules|6.0|\n",
      "|62948792-f794-45a...|  1.0|IndianTeenagers|2024-10-19T16:00:14|âœ¨MOST AWAITED SCH...|Hellll-lllowwwwðŸ˜š...|        public|                     77252|https://www.reddi...|2024-10-19T15:13:53|      0.67|  thy|         -sxmxd|0.0|\n",
      "+--------------------+-----+---------------+-------------------+--------------------+--------------------+--------------+--------------------------+--------------------+-------------------+----------+-----+---------------+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"filename\", StringType(), True),\n",
    "    StructField(\"SCORE\", FloatType(), True),\n",
    "    StructField(\"SUBREDDIT\", StringType(), True),\n",
    "    StructField(\"ETL_DATE\" ,StringType(), True),\n",
    "    StructField(\"TITLE\", StringType(), True),\n",
    "    StructField(\"TEXT\",StringType(), True),\n",
    "    StructField(\"SUBREDDIT_TYPE\",StringType(), True),\n",
    "    StructField(\"SUBREDDIT_SUBSCRIBER_COUNT\",IntegerType(), True),\n",
    "    StructField(\"URL\" ,StringType(), True),\n",
    "    StructField(\"TS\" ,StringType(), True),\n",
    "    StructField(\"VOTE_RATIO\",FloatType(), True),\n",
    "    StructField(\"TOPIC\",StringType(), True),\n",
    "    StructField(\"USER_NAME\",StringType(), True),\n",
    "    StructField(\"WLS\",FloatType(), True)\n",
    "    ])\n",
    "\n",
    "\n",
    "#df_parquet= spark.read.parquet(\"s3a://reddit/*.parquet\", inferSchema=True, schema=schema)\n",
    "df= spark.read.parquet(\"s3a://reddit/*\", inferSchema=True, schema=schema)\n",
    "\n",
    "df.show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           USER_NAME|count|\n",
      "+--------------------+-----+\n",
      "|       CabianD_uTest|  985|\n",
      "|         ar_david_hh|  399|\n",
      "|     madscientist174|  379|\n",
      "|       Faction_Chief|  376|\n",
      "| subredditsummarybot|  375|\n",
      "|   Far-Elephant-2612|  363|\n",
      "|TearRepresentative56|  318|\n",
      "|       AutoModerator|  312|\n",
      "|           GiversBot|  293|\n",
      "|       LivinAmiracle|  253|\n",
      "|ThisIsARealAccountAP|  249|\n",
      "|    Annabelle-Surely|  242|\n",
      "|         OolongSippy|  238|\n",
      "|           bmasumian|  235|\n",
      "|             aproyal|  230|\n",
      "|               MRGDN|  216|\n",
      "|       xMysticChimez|  215|\n",
      "|          BigBaibars|  206|\n",
      "|              Fiff02|  206|\n",
      "|        SkipperDipps|  202|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "most_write_user = df\\\n",
    "    .filter(df.USER_NAME.isNotNull())\\\n",
    "    .groupBy(\"USER_NAME\")\\\n",
    "    .count()\\\n",
    "    .orderBy(\"count\", ascending=False).coalesce(2)\n",
    "most_write_user.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=====================================================>  (19 + 1) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      date|count|\n",
      "+----------+-----+\n",
      "|2024-10-18| 7200|\n",
      "|2024-10-17| 7200|\n",
      "|2024-10-19| 6993|\n",
      "|2024-10-16| 7200|\n",
      "|2024-10-15| 7200|\n",
      "|2024-10-13| 4550|\n",
      "|2024-10-14| 7200|\n",
      "|2024-10-23| 1200|\n",
      "|2024-10-24| 1166|\n",
      "|2024-10-22| 1200|\n",
      "|2024-10-20| 1192|\n",
      "|2024-10-26| 1102|\n",
      "|2024-10-21| 1200|\n",
      "|2024-10-25| 1200|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "daily_message_df = df.groupBy(date_format(\"ETL_DATE\", \"yyyy-MM-dd\").alias(\"date\")).count()\n",
    "daily_message_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:=====================================================>  (19 + 1) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+\n",
      "|      date|           USER_NAME|count|\n",
      "+----------+--------------------+-----+\n",
      "|2024-10-19|      TheMightyBox72|   23|\n",
      "|2024-10-19|Unique_Homework_4065|   23|\n",
      "|2024-10-18| HiMaintainceMachine|   14|\n",
      "|2024-10-19|         TheLight123|   14|\n",
      "|2024-10-19|        EarthPuma120|    5|\n",
      "|2024-10-19|     Unable_Pool3236|    7|\n",
      "|2024-10-19|            catronex|   24|\n",
      "|2024-10-17|       Faction_Chief|   25|\n",
      "|2024-10-17|         PeterLoew88|   24|\n",
      "|2024-10-18|       Faction_Chief|   55|\n",
      "|2024-10-19|        Neon_Genisis|    7|\n",
      "|2024-10-19|Regular_Software_452|   21|\n",
      "|2024-10-17|           Khrizizzo|   24|\n",
      "|2024-10-17|   Medical_Poet_5490|   24|\n",
      "|2024-10-19|              -sxmxd|    7|\n",
      "|2024-10-18|               MRGDN|   48|\n",
      "|2024-10-19|          steve32767|   24|\n",
      "|2024-10-19|    GameProfessional|    6|\n",
      "|2024-10-19|         ar_david_hh|   77|\n",
      "|2024-10-19|        teaquiladiva|   23|\n",
      "+----------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "most_subscriber_per_day_df = df.groupBy(date_format(\"ETL_DATE\", \"yyyy-MM-dd\").alias(\"date\"),\n",
    "                                df.USER_NAME\n",
    "                                ).count()\n",
    "most_subscriber_per_day_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:==================================================>     (18 + 1) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| topic|count|\n",
      "+------+-----+\n",
      "|   thy|24310|\n",
      "|  cars|15819|\n",
      "|turkey|15674|\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "topic_count = df.groupBy(\"topic\").count()\n",
    "topic_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:=====================================================>  (19 + 1) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+\n",
      "| topic|        AVG_SCORE|\n",
      "+------+-----------------+\n",
      "|   thy|75.28918140682846|\n",
      "|  cars|19.44433908590935|\n",
      "|turkey|78.55040193951767|\n",
      "+------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "avg_score_per_topic = df.groupBy(\"topic\").avg(\"SCORE\").withColumnRenamed(\"avg(SCORE)\", \"AVG_SCORE\")\n",
    "avg_score_per_topic.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG_NAME = \"nessie\"\n",
    "DB_NAME = \"reddit\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE NAMESPACE IF NOT EXISTS {CATALOG_NAME}.{DB_NAME};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TABLE_NAME = \"daily_message\"\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {CATALOG_NAME}.{DB_NAME}.{TABLE_NAME}_withpartitions (\n",
    "        date   DATE,\n",
    "        count   INTEGER\n",
    "        )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (date);\n",
    "    \"\"\")\n",
    "\n",
    "#--------------------------------------------------------------------------#\n",
    "\n",
    "TABLE_NAME = \"most_write_user\"\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {CATALOG_NAME}.{DB_NAME}.{TABLE_NAME}_withpartitions (\n",
    "        USER_NAME   STRING,\n",
    "        count   INTEGER\n",
    "        )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (USER_NAME);\n",
    "    \"\"\")\n",
    "\n",
    "#--------------------------------------------------------------------------#\n",
    "\n",
    "TABLE_NAME = \"topic_count\"\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {CATALOG_NAME}.{DB_NAME}.{TABLE_NAME} (\n",
    "        topic   STRING,\n",
    "        count   INTEGER\n",
    "        )\n",
    "    USING iceberg\n",
    "    \"\"\")\n",
    "\n",
    "#--------------------------------------------------------------------------#\n",
    "\n",
    "TABLE_NAME = \"most_subscriber_per_day\"\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {CATALOG_NAME}.{DB_NAME}.{TABLE_NAME}_withpartitions (\n",
    "        date   TIMESTAMP,\n",
    "        USER_NAME   STRING,\n",
    "        count   INTEGER\n",
    "        )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (date);\n",
    "    \"\"\")\n",
    "\n",
    "#--------------------------------------------------------------------------#\n",
    "\n",
    "TABLE_NAME = \"avg_score_per_topic\"\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {CATALOG_NAME}.{DB_NAME}.{TABLE_NAME} (\n",
    "        topic   STRING,\n",
    "        AVG_SCORE   DOUBLE\n",
    "        )\n",
    "    USING iceberg\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# unpartitioned tables\n",
    "avg_score_per_topic.write.mode(\"append\").insertInto(f\"{CATALOG_NAME}.{DB_NAME}.avg_score_per_topic\") \n",
    "\n",
    "topic_count.write.mode(\"append\").insertInto(f\"{CATALOG_NAME}.{DB_NAME}.topic_count\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#partitioned tables\n",
    "\n",
    "most_subscriber_per_day_df\\\n",
    ".withColumn(\"date\",most_subscriber_per_day_df['date'].cast(DateType()))\\\n",
    ".orderBy(\"date\")\\\n",
    ".writeTo(f\"{CATALOG_NAME}.{DB_NAME}.most_subscriber_per_day_withpartitions\").append()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "most_write_user.orderBy(\"USER_NAME\")\\\n",
    "    .write.mode(\"append\").insertInto(f\"{CATALOG_NAME}.{DB_NAME}.most_write_user_withpartitions\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_message_df.orderBy(\"date\")\\\n",
    ".withColumn(\"date\",daily_message_df['date'].cast(DateType()))\\\n",
    ".orderBy(\"date\")\\\n",
    "    .write.mode(\"append\").insertInto(f\"{CATALOG_NAME}.{DB_NAME}.daily_message_withpartitions\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create branch\n",
    "URL = f\"{CATALOG_NAME}.{DB_NAME}.daily_message_withpartitions\"\n",
    "print(URL)\n",
    "spark.sql(f\"ALTER TABLE {URL} CREATE BRANCH development\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "daily_message_df\\\n",
    ".withColumn(\"date\",daily_message_df['date'].cast(DateType())).write.format(\"iceberg\")\\\n",
    "    .option(\"branch\", \"development\")\\\n",
    "    .mode(\"append\")\\\n",
    "    .save(f\"{CATALOG_NAME}.{DB_NAME}.daily_message_withpartitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:==============================================>          (9 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      date|count|\n",
      "+----------+-----+\n",
      "|2024-10-26| 1102|\n",
      "|2024-10-25| 1200|\n",
      "|2024-10-24| 1166|\n",
      "|2024-10-23| 1200|\n",
      "|2024-10-22| 1200|\n",
      "|2024-10-22| 1200|\n",
      "|2024-10-22| 1200|\n",
      "|2024-10-21| 1200|\n",
      "|2024-10-21| 1200|\n",
      "|2024-10-21| 1200|\n",
      "|2024-10-20| 1192|\n",
      "|2024-10-20| 1192|\n",
      "|2024-10-20| 1124|\n",
      "|2024-10-20| 1192|\n",
      "|2024-10-19| 6993|\n",
      "|2024-10-19| 6993|\n",
      "|2024-10-19| 6993|\n",
      "|2024-10-19| 6993|\n",
      "|2024-10-18| 7200|\n",
      "|2024-10-18| 7200|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM nessie.reddit.daily_message_withpartitions VERSION AS OF 'development' ORDER BY date DESC ;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete branch\n",
    "#spark.sql(\"ALTER TABLE glue.test.employees DROP BRANCH ML_exp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"ALTER TABLE {CATALOG_NAME}.{DB_NAME}.daily_message_withpartitions CREATE TAG 22_OCT_REPORT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      date|count|\n",
      "+----------+-----+\n",
      "|2024-10-13| 4550|\n",
      "|2024-10-16| 7200|\n",
      "|2024-10-17| 7200|\n",
      "|2024-10-14| 7200|\n",
      "|2024-10-15| 7200|\n",
      "|2024-10-20| 1124|\n",
      "|2024-10-18| 7200|\n",
      "|2024-10-19| 6993|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM  \n",
    "    nessie.reddit.daily_message_withpartitions.tag_22_OCT_REPORT\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2024-10-20 16:35:...|7759802095667761594|               NULL|               true|\n",
      "|2024-10-22 22:16:...|4048158043975537361|7759802095667761594|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"SELECT * FROM {CATALOG_NAME}.{DB_NAME}.daily_message_withpartitions.history;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snapshoots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|2024-10-20 16:35:...|7759802095667761594|               NULL|   append|s3a://warehouse/r...|{spark.app.id -> ...|\n",
      "|2024-10-22 22:16:...|4048158043975537361|7759802095667761594|   append|s3a://warehouse/r...|{spark.app.id -> ...|\n",
      "|2024-10-22 22:31:...|7580483639421632325|7759802095667761594|   append|s3a://warehouse/r...|{spark.app.id -> ...|\n",
      "|2024-10-22 22:31:...|2404012265844688689|7580483639421632325|   append|s3a://warehouse/r...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"SELECT * FROM {CATALOG_NAME}.{DB_NAME}.daily_message_withpartitions.snapshots;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      date|count|\n",
      "+----------+-----+\n",
      "|2024-10-13| 4550|\n",
      "|2024-10-13| 4550|\n",
      "|2024-10-16| 7200|\n",
      "|2024-10-16| 7200|\n",
      "|2024-10-17| 7200|\n",
      "|2024-10-17| 7200|\n",
      "|2024-10-14| 7200|\n",
      "|2024-10-14| 7200|\n",
      "|2024-10-15| 7200|\n",
      "|2024-10-15| 7200|\n",
      "|2024-10-20| 1192|\n",
      "|2024-10-20| 1124|\n",
      "|2024-10-21| 1200|\n",
      "|2024-10-18| 7200|\n",
      "|2024-10-18| 7200|\n",
      "|2024-10-19| 6993|\n",
      "|2024-10-19| 6993|\n",
      "|2024-10-22| 1200|\n",
      "|2024-10-13| 4550|\n",
      "|2024-10-16| 7200|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"SELECT * FROM {CATALOG_NAME}.{DB_NAME}.daily_message_withpartitions VERSION AS OF 2404012265844688689;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
